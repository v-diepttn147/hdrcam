// @generated by tools/codegen/gen.py from DispatchKeyFunctions.h

// NB: The implementing C++ file is RegisterDispatchKey.cpp

// TODO: tighten this include
#include <ATen/Functions.h>

namespace at {
namespace math {

TORCH_API Tensor _cast_Byte(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _cast_Char(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _cast_Double(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _cast_Float(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _cast_Int(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _cast_Long(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _cast_Short(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _cast_Half(const Tensor & self, bool non_blocking=false);
TORCH_API Tensor _make_dual(const Tensor & primal, const Tensor & tangent, int64_t level);
TORCH_API std::tuple<Tensor,Tensor> _unpack_dual(const Tensor & dual, int64_t level);
TORCH_API Tensor & rename_(Tensor & self, c10::optional<DimnameList> names);
TORCH_API Tensor rename(const Tensor & self, c10::optional<DimnameList> names);
TORCH_API Tensor align_to(const Tensor & self, DimnameList names);
TORCH_API Tensor align_to(const Tensor & self, DimnameList order, int64_t ellipsis_idx);
TORCH_API Tensor align_as(const Tensor & self, const Tensor & other);
TORCH_API std::vector<Tensor> align_tensors(TensorList tensors);
TORCH_API Tensor refine_names(const Tensor & self, DimnameList names);
TORCH_API bool _use_cudnn_rnn_flatten_weight();
TORCH_API int64_t _debug_has_internal_overlap(const Tensor & self);
TORCH_API std::tuple<Tensor,Tensor> _sobol_engine_draw(const Tensor & quasi, int64_t n, const Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<ScalarType> dtype);
TORCH_API Tensor & _sobol_engine_ff_(Tensor & self, int64_t n, const Tensor & sobolstate, int64_t dimension, int64_t num_generated);
TORCH_API Tensor & _sobol_engine_scramble_(Tensor & self, const Tensor & ltm, int64_t dimension);
TORCH_API Tensor & _sobol_engine_initialize_state_(Tensor & self, int64_t dimension);
TORCH_API Tensor _reshape_from_tensor(const Tensor & self, const Tensor & shape);
TORCH_API Tensor _shape_as_tensor(const Tensor & self);
TORCH_API Tensor dropout(const Tensor & input, double p, bool train);
TORCH_API Tensor & dropout_(Tensor & self, double p, bool train);
TORCH_API Tensor feature_dropout(const Tensor & input, double p, bool train);
TORCH_API Tensor & feature_dropout_(Tensor & self, double p, bool train);
TORCH_API Tensor alpha_dropout(const Tensor & input, double p, bool train);
TORCH_API Tensor & alpha_dropout_(Tensor & self, double p, bool train);
TORCH_API Tensor feature_alpha_dropout(const Tensor & input, double p, bool train);
TORCH_API Tensor & feature_alpha_dropout_(Tensor & self, double p, bool train);
TORCH_API Tensor absolute(const Tensor & self);
TORCH_API Tensor & absolute_(Tensor & self);
TORCH_API Tensor & absolute_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & absolute_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor real(const Tensor & self);
TORCH_API Tensor imag(const Tensor & self);
TORCH_API Tensor conj(const Tensor & self);
TORCH_API Tensor arccos(const Tensor & self);
TORCH_API Tensor & arccos_(Tensor & self);
TORCH_API Tensor & arccos_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & arccos_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor avg_pool1d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride={}, IntArrayRef padding=0, bool ceil_mode=false, bool count_include_pad=true);
TORCH_API Tensor adaptive_avg_pool1d(const Tensor & self, IntArrayRef output_size);
TORCH_API std::tuple<Tensor,Tensor> adaptive_max_pool1d(const Tensor & self, IntArrayRef output_size);
TORCH_API Tensor addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, const Scalar & beta=1, const Scalar & alpha=1);
TORCH_API Tensor & addr_out(Tensor & out, const Tensor & self, const Tensor & vec1, const Tensor & vec2, const Scalar & beta=1, const Scalar & alpha=1);
TORCH_API Tensor & addr_outf(const Tensor & self, const Tensor & vec1, const Tensor & vec2, const Scalar & beta, const Scalar & alpha, Tensor & out);
TORCH_API Tensor affine_grid_generator_backward(const Tensor & grad, IntArrayRef size, bool align_corners);
TORCH_API Tensor all(const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API Tensor & all_out(Tensor & out, const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API Tensor & all_outf(const Tensor & self, Dimname dim, bool keepdim, Tensor & out);
TORCH_API bool allclose(const Tensor & self, const Tensor & other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false);
TORCH_API Tensor any(const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API Tensor & any_out(Tensor & out, const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API Tensor & any_outf(const Tensor & self, Dimname dim, bool keepdim, Tensor & out);
TORCH_API Tensor arange(const Scalar & end, TensorOptions options={});
TORCH_API Tensor arange(const Scalar & end, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor arange(const Scalar & start, const Scalar & end, TensorOptions options={});
TORCH_API Tensor arange(const Scalar & start, const Scalar & end, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor arange(const Scalar & start, const Scalar & end, const Scalar & step, TensorOptions options={});
TORCH_API Tensor arange(const Scalar & start, const Scalar & end, const Scalar & step, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & arange_out(Tensor & out, const Scalar & end);
TORCH_API Tensor & arange_outf(const Scalar & end, Tensor & out);
TORCH_API Tensor _dim_arange(const Tensor & like, int64_t dim);
TORCH_API Tensor arccosh(const Tensor & self);
TORCH_API Tensor & arccosh_(Tensor & self);
TORCH_API Tensor & arccosh_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & arccosh_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor arcsinh(const Tensor & self);
TORCH_API Tensor & arcsinh_(Tensor & self);
TORCH_API Tensor & arcsinh_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & arcsinh_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor arctanh(const Tensor & self);
TORCH_API Tensor & arctanh_(Tensor & self);
TORCH_API Tensor & arctanh_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & arctanh_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor arcsin(const Tensor & self);
TORCH_API Tensor & arcsin_(Tensor & self);
TORCH_API Tensor & arcsin_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & arcsin_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor arctan(const Tensor & self);
TORCH_API Tensor & arctan_(Tensor & self);
TORCH_API Tensor & arctan_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & arctan_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor atleast_1d(const Tensor & self);
TORCH_API std::vector<Tensor> atleast_1d(TensorList tensors);
TORCH_API Tensor atleast_2d(const Tensor & self);
TORCH_API std::vector<Tensor> atleast_2d(TensorList tensors);
TORCH_API Tensor atleast_3d(const Tensor & self);
TORCH_API std::vector<Tensor> atleast_3d(TensorList tensors);
TORCH_API Tensor & _baddbmm_mkl_(Tensor & self, const Tensor & batch1, const Tensor & batch2, const Scalar & beta=1, const Scalar & alpha=1);
TORCH_API Tensor bartlett_window(int64_t window_length, TensorOptions options={});
TORCH_API Tensor bartlett_window(int64_t window_length, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor bartlett_window(int64_t window_length, bool periodic, TensorOptions options={});
TORCH_API Tensor bartlett_window(int64_t window_length, bool periodic, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor batch_norm(const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled);
TORCH_API std::tuple<Tensor,Tensor,Tensor,Tensor,int64_t> _batch_norm_impl_index(const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled);
TORCH_API std::tuple<Tensor,Tensor,Tensor> _batch_norm_impl_index_backward(int64_t impl_index, const Tensor & input, const Tensor & grad_output, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, const c10::optional<Tensor> & save_mean, const c10::optional<Tensor> & save_var_transform, bool train, double eps, std::array<bool,3> output_mask, const Tensor & reservedSpace);
TORCH_API Tensor bernoulli(const Tensor & self, double p, c10::optional<Generator> generator=c10::nullopt);
TORCH_API Tensor bilinear(const Tensor & input1, const Tensor & input2, const Tensor & weight, const c10::optional<Tensor> & bias);
TORCH_API Tensor binary_cross_entropy_with_logits_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight={}, const c10::optional<Tensor> & pos_weight={}, int64_t reduction=at::Reduction::Mean);
TORCH_API Tensor bitwise_not(const Tensor & self);
TORCH_API Tensor & bitwise_not_(Tensor & self);
TORCH_API Tensor logical_not(const Tensor & self);
TORCH_API Tensor & logical_not_(Tensor & self);
TORCH_API Tensor logical_xor(const Tensor & self, const Tensor & other);
TORCH_API Tensor & logical_xor_(Tensor & self, const Tensor & other);
TORCH_API Tensor logical_and(const Tensor & self, const Tensor & other);
TORCH_API Tensor & logical_and_(Tensor & self, const Tensor & other);
TORCH_API Tensor logical_or(const Tensor & self, const Tensor & other);
TORCH_API Tensor & logical_or_(Tensor & self, const Tensor & other);
TORCH_API Tensor blackman_window(int64_t window_length, TensorOptions options={});
TORCH_API Tensor blackman_window(int64_t window_length, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor blackman_window(int64_t window_length, bool periodic, TensorOptions options={});
TORCH_API Tensor blackman_window(int64_t window_length, bool periodic, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API std::vector<Tensor> broadcast_tensors(TensorList tensors);
TORCH_API Tensor broadcast_to(const Tensor & self, IntArrayRef size);
TORCH_API Tensor cat(TensorList tensors, Dimname dim);
TORCH_API Tensor & cat_out(Tensor & out, TensorList tensors, Dimname dim);
TORCH_API Tensor & cat_outf(TensorList tensors, Dimname dim, Tensor & out);
TORCH_API Tensor block_diag(TensorList tensors);
TORCH_API Tensor chain_matmul(TensorList matrices);
TORCH_API std::vector<Tensor> unsafe_chunk(const Tensor & self, int64_t chunks, int64_t dim=0);
TORCH_API std::vector<Tensor> chunk(const Tensor & self, int64_t chunks, int64_t dim=0);
TORCH_API std::vector<Tensor> tensor_split(const Tensor & self, int64_t sections, int64_t dim=0);
TORCH_API std::vector<Tensor> tensor_split(const Tensor & self, IntArrayRef indices, int64_t dim=0);
TORCH_API std::vector<Tensor> tensor_split(const Tensor & self, const Tensor & tensor_indices_or_sections, int64_t dim=0);
TORCH_API Tensor clip(const Tensor & self, const c10::optional<Scalar> & min=c10::nullopt, const c10::optional<Scalar> & max=c10::nullopt);
TORCH_API Tensor & clip_(Tensor & self, const c10::optional<Scalar> & min=c10::nullopt, const c10::optional<Scalar> & max=c10::nullopt);
TORCH_API Tensor & clip_out(Tensor & out, const Tensor & self, const c10::optional<Scalar> & min=c10::nullopt, const c10::optional<Scalar> & max=c10::nullopt);
TORCH_API Tensor & clip_outf(const Tensor & self, const c10::optional<Scalar> & min, const c10::optional<Scalar> & max, Tensor & out);
TORCH_API bool cudnn_is_acceptable(const Tensor & self);
TORCH_API Tensor contiguous(const Tensor & self, MemoryFormat memory_format=MemoryFormat::Contiguous);
TORCH_API Tensor convolution(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups);
TORCH_API Tensor _convolution(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32);
TORCH_API Tensor _convolution(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled);
TORCH_API Tensor _convolution_nogroup(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding);
TORCH_API std::tuple<Tensor,Tensor,Tensor> _convolution_double_backward(const c10::optional<Tensor> & ggI, const c10::optional<Tensor> & ggW, const c10::optional<Tensor> & ggb, const Tensor & gO, const Tensor & weight, const Tensor & self, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, std::array<bool,3> output_mask);
TORCH_API Tensor conv1d(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef dilation=1, int64_t groups=1);
TORCH_API Tensor conv2d(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef dilation=1, int64_t groups=1);
TORCH_API Tensor conv3d(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef dilation=1, int64_t groups=1);
TORCH_API std::tuple<Tensor,Tensor,Tensor> conv_tbc_backward(const Tensor & self, const Tensor & input, const Tensor & weight, const Tensor & bias, int64_t pad);
TORCH_API Tensor conv_transpose1d(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef output_padding=0, int64_t groups=1, IntArrayRef dilation=1);
TORCH_API Tensor conv_transpose2d(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef output_padding=0, int64_t groups=1, IntArrayRef dilation=1);
TORCH_API Tensor conv_transpose3d(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef output_padding=0, int64_t groups=1, IntArrayRef dilation=1);
TORCH_API Tensor cosine_embedding_loss(const Tensor & input1, const Tensor & input2, const Tensor & target, double margin=0.0, int64_t reduction=at::Reduction::Mean);
TORCH_API std::tuple<Tensor,Tensor> cummax(const Tensor & self, Dimname dim);
TORCH_API std::tuple<Tensor &,Tensor &> cummax_out(Tensor & values, Tensor & indices, const Tensor & self, Dimname dim);
TORCH_API std::tuple<Tensor &,Tensor &> cummax_outf(const Tensor & self, Dimname dim, Tensor & values, Tensor & indices);
TORCH_API std::tuple<Tensor,Tensor> cummin(const Tensor & self, Dimname dim);
TORCH_API std::tuple<Tensor &,Tensor &> cummin_out(Tensor & values, Tensor & indices, const Tensor & self, Dimname dim);
TORCH_API std::tuple<Tensor &,Tensor &> cummin_outf(const Tensor & self, Dimname dim, Tensor & values, Tensor & indices);
TORCH_API Tensor cummaxmin_backward(const Tensor & grad, const Tensor & input, const Tensor & indices, int64_t dim);
TORCH_API Tensor cumprod(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & cumprod_(Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & cumprod_out(Tensor & out, const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & cumprod_outf(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype, Tensor & out);
TORCH_API Tensor cumprod_backward(const Tensor & grad, const Tensor & input, int64_t dim, const Tensor & output);
TORCH_API Tensor cumsum(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & cumsum_(Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & cumsum_out(Tensor & out, const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & cumsum_outf(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype, Tensor & out);
TORCH_API Tensor ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank=0, int64_t reduction=at::Reduction::Mean, bool zero_infinity=false);
TORCH_API Tensor ctc_loss(const Tensor & log_probs, const Tensor & targets, const Tensor & input_lengths, const Tensor & target_lengths, int64_t blank=0, int64_t reduction=at::Reduction::Mean, bool zero_infinity=false);
TORCH_API Tensor diag_embed(const Tensor & self, int64_t offset=0, int64_t dim1=-2, int64_t dim2=-1);
TORCH_API Tensor diagflat(const Tensor & self, int64_t offset=0);
TORCH_API Tensor diagonal(const Tensor & self, Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset=0);
TORCH_API Tensor diagonal_backward(const Tensor & grad, IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2);
TORCH_API Tensor & fill_diagonal_(Tensor & self, const Scalar & fill_value, bool wrap=false);
TORCH_API Tensor diff(const Tensor & self, int64_t n=1, int64_t dim=-1, const c10::optional<Tensor> & prepend={}, const c10::optional<Tensor> & append={});
TORCH_API Tensor & diff_out(Tensor & out, const Tensor & self, int64_t n=1, int64_t dim=-1, const c10::optional<Tensor> & prepend={}, const c10::optional<Tensor> & append={});
TORCH_API Tensor & diff_outf(const Tensor & self, int64_t n, int64_t dim, const c10::optional<Tensor> & prepend, const c10::optional<Tensor> & append, Tensor & out);
TORCH_API Tensor divide(const Tensor & self, const Tensor & other);
TORCH_API Tensor & divide_(Tensor & self, const Tensor & other);
TORCH_API Tensor & divide_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & divide_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor divide(const Tensor & self, const Scalar & other);
TORCH_API Tensor & divide_(Tensor & self, const Scalar & other);
TORCH_API Tensor divide(const Tensor & self, const Tensor & other, std::string rounding_mode);
TORCH_API Tensor & divide_(Tensor & self, const Tensor & other, std::string rounding_mode);
TORCH_API Tensor & divide_out(Tensor & out, const Tensor & self, const Tensor & other, std::string rounding_mode);
TORCH_API Tensor & divide_outf(const Tensor & self, const Tensor & other, std::string rounding_mode, Tensor & out);
TORCH_API Tensor divide(const Tensor & self, const Scalar & other, std::string rounding_mode);
TORCH_API Tensor & divide_(Tensor & self, const Scalar & other, std::string rounding_mode);
TORCH_API Tensor true_divide(const Tensor & self, const Tensor & other);
TORCH_API Tensor & true_divide_(Tensor & self, const Tensor & other);
TORCH_API Tensor & true_divide_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & true_divide_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor true_divide(const Tensor & self, const Scalar & other);
TORCH_API Tensor & true_divide_(Tensor & self, const Scalar & other);
TORCH_API Tensor einsum(std::string equation, TensorList tensors);
TORCH_API Tensor embedding_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse);
TORCH_API Tensor embedding_sparse_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq);
TORCH_API std::tuple<Tensor,Tensor> _rowwise_prune(const Tensor & weight, const Tensor & mask, ScalarType compressed_indices_dtype);
TORCH_API Tensor row_stack(TensorList tensors);
TORCH_API Tensor & row_stack_out(Tensor & out, TensorList tensors);
TORCH_API Tensor & row_stack_outf(TensorList tensors, Tensor & out);
TORCH_API std::tuple<Tensor,Tensor,Tensor,Tensor> embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq=false, int64_t mode=0, bool sparse=false, const c10::optional<Tensor> & per_sample_weights={}, bool include_last_offset=false);
TORCH_API Tensor _embedding_bag_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<Tensor> & per_sample_weights);
TORCH_API Tensor _embedding_bag_sparse_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<Tensor> & per_sample_weights);
TORCH_API Tensor empty(IntArrayRef size, c10::optional<DimnameList> names, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor empty(IntArrayRef size, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor new_empty(const Tensor & self, IntArrayRef size, TensorOptions options={});
TORCH_API Tensor new_empty(const Tensor & self, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor new_empty_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, TensorOptions options={});
TORCH_API Tensor new_empty_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor new_full(const Tensor & self, IntArrayRef size, const Scalar & fill_value, TensorOptions options={});
TORCH_API Tensor new_full(const Tensor & self, IntArrayRef size, const Scalar & fill_value, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor new_zeros(const Tensor & self, IntArrayRef size, TensorOptions options={});
TORCH_API Tensor new_zeros(const Tensor & self, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & empty_out(Tensor & out, IntArrayRef size, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor & empty_outf(IntArrayRef size, c10::optional<MemoryFormat> memory_format, Tensor & out);
TORCH_API Tensor empty_like(const Tensor & self, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor empty_like(const Tensor & self, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor expand_as(const Tensor & self, const Tensor & other);
TORCH_API Tensor eye(int64_t n, TensorOptions options={});
TORCH_API Tensor eye(int64_t n, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor eye(int64_t n, int64_t m, TensorOptions options={});
TORCH_API Tensor eye(int64_t n, int64_t m, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor flatten(const Tensor & self, int64_t start_dim=0, int64_t end_dim=-1);
TORCH_API Tensor flatten(const Tensor & self, int64_t start_dim, int64_t end_dim, Dimname out_dim);
TORCH_API Tensor flatten(const Tensor & self, Dimname start_dim, Dimname end_dim, Dimname out_dim);
TORCH_API Tensor flatten(const Tensor & self, DimnameList dims, Dimname out_dim);
TORCH_API Tensor unflatten(const Tensor & self, int64_t dim, IntArrayRef sizes, c10::optional<DimnameList> names=c10::nullopt);
TORCH_API Tensor unflatten(const Tensor & self, Dimname dim, IntArrayRef sizes, DimnameList names);
TORCH_API Tensor floor_divide(const Tensor & self, const Scalar & other);
TORCH_API Tensor & floor_divide_(Tensor & self, const Scalar & other);
TORCH_API Tensor full(IntArrayRef size, const Scalar & fill_value, c10::optional<DimnameList> names, TensorOptions options={});
TORCH_API Tensor full(IntArrayRef size, const Scalar & fill_value, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor full(IntArrayRef size, const Scalar & fill_value, TensorOptions options={});
TORCH_API Tensor full(IntArrayRef size, const Scalar & fill_value, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & full_out(Tensor & out, IntArrayRef size, const Scalar & fill_value);
TORCH_API Tensor & full_outf(IntArrayRef size, const Scalar & fill_value, Tensor & out);
TORCH_API Tensor full_like(const Tensor & self, const Scalar & fill_value, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor full_like(const Tensor & self, const Scalar & fill_value, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor gcd(const Tensor & self, const Tensor & other);
TORCH_API Tensor & gcd_(Tensor & self, const Tensor & other);
TORCH_API Tensor lcm(const Tensor & self, const Tensor & other);
TORCH_API Tensor & lcm_(Tensor & self, const Tensor & other);
TORCH_API Tensor grid_sampler(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners);
TORCH_API std::tuple<Tensor,Tensor> _grid_sampler_2d_cpu_fallback_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners);
TORCH_API Tensor hann_window(int64_t window_length, TensorOptions options={});
TORCH_API Tensor hann_window(int64_t window_length, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor hann_window(int64_t window_length, bool periodic, TensorOptions options={});
TORCH_API Tensor hann_window(int64_t window_length, bool periodic, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor hamming_window(int64_t window_length, TensorOptions options={});
TORCH_API Tensor hamming_window(int64_t window_length, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor hamming_window(int64_t window_length, bool periodic, TensorOptions options={});
TORCH_API Tensor hamming_window(int64_t window_length, bool periodic, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor hamming_window(int64_t window_length, bool periodic, double alpha, TensorOptions options={});
TORCH_API Tensor hamming_window(int64_t window_length, bool periodic, double alpha, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor hamming_window(int64_t window_length, bool periodic, double alpha, double beta, TensorOptions options={});
TORCH_API Tensor hamming_window(int64_t window_length, bool periodic, double alpha, double beta, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor kaiser_window(int64_t window_length, TensorOptions options={});
TORCH_API Tensor kaiser_window(int64_t window_length, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor kaiser_window(int64_t window_length, bool periodic, TensorOptions options={});
TORCH_API Tensor kaiser_window(int64_t window_length, bool periodic, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor kaiser_window(int64_t window_length, bool periodic, double beta, TensorOptions options={});
TORCH_API Tensor kaiser_window(int64_t window_length, bool periodic, double beta, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor hinge_embedding_loss(const Tensor & self, const Tensor & target, double margin=1.0, int64_t reduction=at::Reduction::Mean);
TORCH_API Tensor group_norm(const Tensor & input, int64_t num_groups, const c10::optional<Tensor> & weight={}, const c10::optional<Tensor> & bias={}, double eps=1e-05, bool cudnn_enabled=true);
TORCH_API std::tuple<Tensor,Tensor,Tensor> native_group_norm(const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps);
TORCH_API int64_t _cufft_get_plan_cache_size(int64_t device_index);
TORCH_API int64_t _cufft_get_plan_cache_max_size(int64_t device_index);
TORCH_API void _cufft_set_plan_cache_max_size(int64_t device_index, int64_t max_size);
TORCH_API void _cufft_clear_plan_cache(int64_t device_index);
TORCH_API Tensor index_copy(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source);
TORCH_API Tensor & index_copy_(Tensor & self, Dimname dim, const Tensor & index, const Tensor & source);
TORCH_API Tensor index_copy(const Tensor & self, Dimname dim, const Tensor & index, const Tensor & source);
TORCH_API Tensor index_put(const Tensor & self, const c10::List<c10::optional<Tensor>> & indices, const Tensor & values, bool accumulate=false);
TORCH_API Tensor instance_norm(const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled);
TORCH_API Tensor isclose(const Tensor & self, const Tensor & other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false);
TORCH_API bool is_distributed(const Tensor & self);
TORCH_API bool is_floating_point(const Tensor & self);
TORCH_API bool is_complex(const Tensor & self);
TORCH_API Tensor isreal(const Tensor & self);
TORCH_API bool is_nonzero(const Tensor & self);
TORCH_API bool is_same_size(const Tensor & self, const Tensor & other);
TORCH_API bool is_signed(const Tensor & self);
TORCH_API Tensor kron(const Tensor & self, const Tensor & other);
TORCH_API Tensor & kron_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & kron_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API std::tuple<Tensor,Tensor> kthvalue(const Tensor & self, int64_t k, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> kthvalue_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> kthvalue_outf(const Tensor & self, int64_t k, Dimname dim, bool keepdim, Tensor & values, Tensor & indices);
TORCH_API Tensor layer_norm(const Tensor & input, IntArrayRef normalized_shape, const c10::optional<Tensor> & weight={}, const c10::optional<Tensor> & bias={}, double eps=1e-05, bool cudnn_enable=true);
TORCH_API std::tuple<Tensor,Tensor,Tensor> native_layer_norm(const Tensor & input, IntArrayRef normalized_shape, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, double eps);
TORCH_API Tensor linear(const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias={});
TORCH_API Tensor fbgemm_linear_int8_weight_fp32_activation(const Tensor & input, const Tensor & weight, const Tensor & packed, const Tensor & col_offsets, const Scalar & weight_scale, const Scalar & weight_zero_point, const Tensor & bias);
TORCH_API Tensor fbgemm_linear_int8_weight(const Tensor & input, const Tensor & weight, const Tensor & packed, const Tensor & col_offsets, const Scalar & weight_scale, const Scalar & weight_zero_point, const Tensor & bias);
TORCH_API std::tuple<Tensor,Tensor,double,int64_t> fbgemm_linear_quantize_weight(const Tensor & input);
TORCH_API Tensor fbgemm_pack_gemm_matrix_fp16(const Tensor & input);
TORCH_API Tensor fbgemm_linear_fp16_weight_fp32_activation(const Tensor & input, const Tensor & packed_weight, const Tensor & bias);
TORCH_API Tensor fbgemm_linear_fp16_weight(const Tensor & input, const Tensor & packed_weight, const Tensor & bias);
TORCH_API Tensor fbgemm_pack_quantized_matrix(const Tensor & input);
TORCH_API Tensor fbgemm_pack_quantized_matrix(const Tensor & input, int64_t K, int64_t N);
TORCH_API Tensor ldexp(const Tensor & self, const Tensor & other);
TORCH_API Tensor & ldexp_(Tensor & self, const Tensor & other);
TORCH_API Tensor & ldexp_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & ldexp_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor linspace(const Scalar & start, const Scalar & end, c10::optional<int64_t> steps=c10::nullopt, TensorOptions options={});
TORCH_API Tensor linspace(const Scalar & start, const Scalar & end, c10::optional<int64_t> steps, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor logspace(const Scalar & start, const Scalar & end, c10::optional<int64_t> steps=c10::nullopt, double base=10.0, TensorOptions options={});
TORCH_API Tensor logspace(const Scalar & start, const Scalar & end, c10::optional<int64_t> steps, double base, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor log_softmax(const Tensor & self, int64_t dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor log_softmax(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor logcumsumexp(const Tensor & self, Dimname dim);
TORCH_API Tensor & logcumsumexp_out(Tensor & out, const Tensor & self, Dimname dim);
TORCH_API Tensor & logcumsumexp_outf(const Tensor & self, Dimname dim, Tensor & out);
TORCH_API Tensor logsumexp(const Tensor & self, DimnameList dim, bool keepdim=false);
TORCH_API Tensor & logsumexp_out(Tensor & out, const Tensor & self, DimnameList dim, bool keepdim=false);
TORCH_API Tensor & logsumexp_outf(const Tensor & self, DimnameList dim, bool keepdim, Tensor & out);
TORCH_API Tensor margin_ranking_loss(const Tensor & input1, const Tensor & input2, const Tensor & target, double margin=0.0, int64_t reduction=at::Reduction::Mean);
TORCH_API Tensor matmul(const Tensor & self, const Tensor & other);
TORCH_API Tensor & matmul_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & matmul_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor matrix_rank(const Tensor & self, double tol, bool symmetric=false);
TORCH_API Tensor matrix_rank(const Tensor & self, bool symmetric=false);
TORCH_API Tensor matrix_power(const Tensor & self, int64_t n);
TORCH_API Tensor matrix_exp_backward(const Tensor & self, const Tensor & grad);
TORCH_API std::tuple<Tensor,Tensor> max(const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> max_out(Tensor & max, Tensor & max_values, const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> max_outf(const Tensor & self, Dimname dim, bool keepdim, Tensor & max, Tensor & max_values);
TORCH_API Tensor value_selecting_reduction_backward(const Tensor & grad, int64_t dim, const Tensor & indices, IntArrayRef sizes, bool keepdim);
TORCH_API std::tuple<Tensor,Tensor> max_pool1d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride={}, IntArrayRef padding=0, IntArrayRef dilation=1, bool ceil_mode=false);
TORCH_API Tensor max_pool1d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride={}, IntArrayRef padding=0, IntArrayRef dilation=1, bool ceil_mode=false);
TORCH_API Tensor max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride={}, IntArrayRef padding=0, IntArrayRef dilation=1, bool ceil_mode=false);
TORCH_API Tensor max_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride={}, IntArrayRef padding=0, IntArrayRef dilation=1, bool ceil_mode=false);
TORCH_API Tensor mean(const Tensor & self, DimnameList dim, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & mean_out(Tensor & out, const Tensor & self, DimnameList dim, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & mean_outf(const Tensor & self, DimnameList dim, bool keepdim, c10::optional<ScalarType> dtype, Tensor & out);
TORCH_API std::tuple<Tensor,Tensor> median(const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> median_out(Tensor & values, Tensor & indices, const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> median_outf(const Tensor & self, Dimname dim, bool keepdim, Tensor & values, Tensor & indices);
TORCH_API std::tuple<Tensor,Tensor> nanmedian(const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> nanmedian_out(Tensor & values, Tensor & indices, const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> nanmedian_outf(const Tensor & self, Dimname dim, bool keepdim, Tensor & values, Tensor & indices);
TORCH_API std::tuple<Tensor,Tensor> min(const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> min_out(Tensor & min, Tensor & min_indices, const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> min_outf(const Tensor & self, Dimname dim, bool keepdim, Tensor & min, Tensor & min_indices);
TORCH_API Tensor mkldnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool bias_defined);
TORCH_API std::tuple<Tensor,Tensor> mkldnn_convolution_backward_weights(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool bias_defined);
TORCH_API Tensor _sparse_mm(const Tensor & sparse, const Tensor & dense);
TORCH_API std::tuple<Tensor,Tensor> mode(const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> mode_out(Tensor & values, Tensor & indices, const Tensor & self, Dimname dim, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> mode_outf(const Tensor & self, Dimname dim, bool keepdim, Tensor & values, Tensor & indices);
TORCH_API Tensor multiply(const Tensor & self, const Tensor & other);
TORCH_API Tensor & multiply_(Tensor & self, const Tensor & other);
TORCH_API Tensor & multiply_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & multiply_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor multiply(const Tensor & self, const Scalar & other);
TORCH_API Tensor & multiply_(Tensor & self, const Scalar & other);
TORCH_API Tensor narrow(const Tensor & self, int64_t dim, int64_t start, int64_t length);
TORCH_API Tensor narrow(const Tensor & self, int64_t dim, const Tensor & start, int64_t length);
TORCH_API bool is_vulkan_available();
TORCH_API bool _nnpack_available();
TORCH_API std::tuple<Tensor,Tensor,Tensor> _nnpack_spatial_convolution_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, std::array<bool,3> output_mask);
TORCH_API Tensor _nnpack_spatial_convolution_backward_input(const Tensor & input, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding);
TORCH_API Tensor _nnpack_spatial_convolution_backward_weight(const Tensor & input, IntArrayRef weightsize, const Tensor & grad_output, IntArrayRef padding);
TORCH_API Tensor ones(IntArrayRef size, c10::optional<DimnameList> names, TensorOptions options={});
TORCH_API Tensor ones(IntArrayRef size, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor ones(IntArrayRef size, TensorOptions options={});
TORCH_API Tensor ones(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & ones_out(Tensor & out, IntArrayRef size);
TORCH_API Tensor & ones_outf(IntArrayRef size, Tensor & out);
TORCH_API Tensor ones_like(const Tensor & self, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor ones_like(const Tensor & self, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor pairwise_distance(const Tensor & x1, const Tensor & x2, double p=2, double eps=1e-06, bool keepdim=false);
TORCH_API Tensor cdist(const Tensor & x1, const Tensor & x2, double p=2, c10::optional<int64_t> compute_mode=c10::nullopt);
TORCH_API Tensor pdist(const Tensor & self, double p=2);
TORCH_API Tensor cosine_similarity(const Tensor & x1, const Tensor & x2, int64_t dim=1, double eps=1e-08);
TORCH_API Tensor movedim(const Tensor & self, IntArrayRef source, IntArrayRef destination);
TORCH_API Tensor movedim(const Tensor & self, int64_t source, int64_t destination);
TORCH_API Tensor moveaxis(const Tensor & self, IntArrayRef source, IntArrayRef destination);
TORCH_API Tensor moveaxis(const Tensor & self, int64_t source, int64_t destination);
TORCH_API Tensor numpy_T(const Tensor & self);
TORCH_API Tensor pixel_shuffle(const Tensor & self, int64_t upscale_factor);
TORCH_API Tensor pixel_unshuffle(const Tensor & self, int64_t downscale_factor);
TORCH_API bool is_pinned(const Tensor & self);
TORCH_API Tensor pin_memory(const Tensor & self);
TORCH_API Tensor pinverse(const Tensor & self, double rcond=1e-15);
TORCH_API Tensor poisson_nll_loss(const Tensor & input, const Tensor & target, bool log_input, bool full, double eps, int64_t reduction);
TORCH_API Tensor scalar_tensor(const Scalar & s, TensorOptions options={});
TORCH_API Tensor scalar_tensor(const Scalar & s, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor rand(IntArrayRef size, c10::optional<DimnameList> names, TensorOptions options={});
TORCH_API Tensor rand(IntArrayRef size, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor rand(IntArrayRef size, c10::optional<Generator> generator, c10::optional<DimnameList> names, TensorOptions options={});
TORCH_API Tensor rand(IntArrayRef size, c10::optional<Generator> generator, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor rand(IntArrayRef size, TensorOptions options={});
TORCH_API Tensor rand(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor rand(IntArrayRef size, c10::optional<Generator> generator, TensorOptions options={});
TORCH_API Tensor rand(IntArrayRef size, c10::optional<Generator> generator, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & rand_out(Tensor & out, IntArrayRef size);
TORCH_API Tensor & rand_outf(IntArrayRef size, Tensor & out);
TORCH_API Tensor & rand_out(Tensor & out, IntArrayRef size, c10::optional<Generator> generator);
TORCH_API Tensor & rand_outf(IntArrayRef size, c10::optional<Generator> generator, Tensor & out);
TORCH_API Tensor rand_like(const Tensor & self, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor rand_like(const Tensor & self, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor randint(int64_t high, IntArrayRef size, TensorOptions options={});
TORCH_API Tensor randint(int64_t high, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor randint(int64_t high, IntArrayRef size, c10::optional<Generator> generator, TensorOptions options={});
TORCH_API Tensor randint(int64_t high, IntArrayRef size, c10::optional<Generator> generator, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor randint(int64_t low, int64_t high, IntArrayRef size, TensorOptions options={});
TORCH_API Tensor randint(int64_t low, int64_t high, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor randint(int64_t low, int64_t high, IntArrayRef size, c10::optional<Generator> generator, TensorOptions options={});
TORCH_API Tensor randint(int64_t low, int64_t high, IntArrayRef size, c10::optional<Generator> generator, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & randint_out(Tensor & out, int64_t high, IntArrayRef size);
TORCH_API Tensor & randint_outf(int64_t high, IntArrayRef size, Tensor & out);
TORCH_API Tensor & randint_out(Tensor & out, int64_t high, IntArrayRef size, c10::optional<Generator> generator);
TORCH_API Tensor & randint_outf(int64_t high, IntArrayRef size, c10::optional<Generator> generator, Tensor & out);
TORCH_API Tensor & randint_out(Tensor & out, int64_t low, int64_t high, IntArrayRef size);
TORCH_API Tensor & randint_outf(int64_t low, int64_t high, IntArrayRef size, Tensor & out);
TORCH_API Tensor & randint_out(Tensor & out, int64_t low, int64_t high, IntArrayRef size, c10::optional<Generator> generator);
TORCH_API Tensor & randint_outf(int64_t low, int64_t high, IntArrayRef size, c10::optional<Generator> generator, Tensor & out);
TORCH_API Tensor randint_like(const Tensor & self, int64_t high, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor randint_like(const Tensor & self, int64_t high, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor randint_like(const Tensor & self, int64_t low, int64_t high, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor randint_like(const Tensor & self, int64_t low, int64_t high, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor randn(IntArrayRef size, TensorOptions options={});
TORCH_API Tensor randn(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor randn(IntArrayRef size, c10::optional<Generator> generator, TensorOptions options={});
TORCH_API Tensor randn(IntArrayRef size, c10::optional<Generator> generator, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor randn(IntArrayRef size, c10::optional<DimnameList> names, TensorOptions options={});
TORCH_API Tensor randn(IntArrayRef size, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor randn(IntArrayRef size, c10::optional<Generator> generator, c10::optional<DimnameList> names, TensorOptions options={});
TORCH_API Tensor randn(IntArrayRef size, c10::optional<Generator> generator, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & randn_out(Tensor & out, IntArrayRef size);
TORCH_API Tensor & randn_outf(IntArrayRef size, Tensor & out);
TORCH_API Tensor & randn_out(Tensor & out, IntArrayRef size, c10::optional<Generator> generator);
TORCH_API Tensor & randn_outf(IntArrayRef size, c10::optional<Generator> generator, Tensor & out);
TORCH_API Tensor randn_like(const Tensor & self, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor randn_like(const Tensor & self, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor randperm(int64_t n, TensorOptions options={});
TORCH_API Tensor randperm(int64_t n, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor randperm(int64_t n, c10::optional<Generator> generator, TensorOptions options={});
TORCH_API Tensor randperm(int64_t n, c10::optional<Generator> generator, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & randperm_out(Tensor & out, int64_t n);
TORCH_API Tensor & randperm_outf(int64_t n, Tensor & out);
TORCH_API Tensor range(const Scalar & start, const Scalar & end, const Scalar & step=1, TensorOptions options={});
TORCH_API Tensor range(const Scalar & start, const Scalar & end, const Scalar & step, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor range(const Scalar & start, const Scalar & end, TensorOptions options={});
TORCH_API Tensor range(const Scalar & start, const Scalar & end, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor ravel(const Tensor & self);
TORCH_API Tensor negative(const Tensor & self);
TORCH_API Tensor & negative_(Tensor & self);
TORCH_API Tensor & negative_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & negative_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor repeat_interleave(const Tensor & self, const Tensor & repeats, c10::optional<int64_t> dim=c10::nullopt);
TORCH_API Tensor repeat_interleave(const Tensor & self, int64_t repeats, c10::optional<int64_t> dim=c10::nullopt);
TORCH_API Tensor reshape(const Tensor & self, IntArrayRef shape);
TORCH_API Tensor reshape_as(const Tensor & self, const Tensor & other);
TORCH_API Tensor rrelu(const Tensor & self, const Scalar & lower=0.125, const Scalar & upper=0.3333333333333333, bool training=false, c10::optional<Generator> generator=c10::nullopt);
TORCH_API Tensor & rrelu_(Tensor & self, const Scalar & lower=0.125, const Scalar & upper=0.3333333333333333, bool training=false, c10::optional<Generator> generator=c10::nullopt);
TORCH_API Tensor relu6(const Tensor & self);
TORCH_API Tensor & relu6_(Tensor & self);
TORCH_API Tensor infinitely_differentiable_gelu_backward(const Tensor & grad, const Tensor & self);
TORCH_API Tensor select(const Tensor & self, Dimname dim, int64_t index);
TORCH_API Tensor select_backward(const Tensor & grad, IntArrayRef input_sizes, int64_t dim, int64_t index);
TORCH_API Tensor selu(const Tensor & self);
TORCH_API Tensor & selu_(Tensor & self);
TORCH_API Tensor silu_backward(const Tensor & grad_output, const Tensor & self);
TORCH_API int64_t size(const Tensor & self, int64_t dim);
TORCH_API int64_t size(const Tensor & self, Dimname dim);
TORCH_API Tensor slice_backward(const Tensor & grad, IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step);
TORCH_API Tensor smm(const Tensor & self, const Tensor & mat2);
TORCH_API Tensor softmax(const Tensor & self, int64_t dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor softmax(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor squeeze(const Tensor & self, Dimname dim);
TORCH_API Tensor & squeeze_(Tensor & self, Dimname dim);
TORCH_API Tensor sspaddmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, const Scalar & beta=1, const Scalar & alpha=1);
TORCH_API Tensor hstack(TensorList tensors);
TORCH_API Tensor & hstack_out(Tensor & out, TensorList tensors);
TORCH_API Tensor & hstack_outf(TensorList tensors, Tensor & out);
TORCH_API Tensor vstack(TensorList tensors);
TORCH_API Tensor & vstack_out(Tensor & out, TensorList tensors);
TORCH_API Tensor & vstack_outf(TensorList tensors, Tensor & out);
TORCH_API Tensor dstack(TensorList tensors);
TORCH_API Tensor & dstack_out(Tensor & out, TensorList tensors);
TORCH_API Tensor & dstack_outf(TensorList tensors, Tensor & out);
TORCH_API Tensor stft(const Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length=c10::nullopt, c10::optional<int64_t> win_length=c10::nullopt, const c10::optional<Tensor> & window={}, bool normalized=false, c10::optional<bool> onesided=c10::nullopt, c10::optional<bool> return_complex=c10::nullopt);
TORCH_API Tensor istft(const Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length=c10::nullopt, c10::optional<int64_t> win_length=c10::nullopt, const c10::optional<Tensor> & window={}, bool center=true, bool normalized=false, c10::optional<bool> onesided=c10::nullopt, c10::optional<int64_t> length=c10::nullopt, bool return_complex=false);
TORCH_API int64_t stride(const Tensor & self, int64_t dim);
TORCH_API int64_t stride(const Tensor & self, Dimname dim);
TORCH_API Tensor sum(const Tensor & self, DimnameList dim, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & sum_out(Tensor & out, const Tensor & self, DimnameList dim, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & sum_outf(const Tensor & self, DimnameList dim, bool keepdim, c10::optional<ScalarType> dtype, Tensor & out);
TORCH_API Tensor sum_to_size(const Tensor & self, IntArrayRef size);
TORCH_API Tensor square(const Tensor & self);
TORCH_API Tensor & square_(Tensor & self);
TORCH_API std::tuple<Tensor,Tensor> std_mean(const Tensor & self, DimnameList dim, bool unbiased=true, bool keepdim=false);
TORCH_API Tensor std(const Tensor & self, DimnameList dim, bool unbiased=true, bool keepdim=false);
TORCH_API Tensor & std_out(Tensor & out, const Tensor & self, DimnameList dim, bool unbiased=true, bool keepdim=false);
TORCH_API Tensor & std_outf(const Tensor & self, DimnameList dim, bool unbiased, bool keepdim, Tensor & out);
TORCH_API Tensor prod(const Tensor & self, Dimname dim, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & prod_out(Tensor & out, const Tensor & self, Dimname dim, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & prod_outf(const Tensor & self, Dimname dim, bool keepdim, c10::optional<ScalarType> dtype, Tensor & out);
TORCH_API Tensor tensordot(const Tensor & self, const Tensor & other, IntArrayRef dims_self, IntArrayRef dims_other);
TORCH_API Tensor tile(const Tensor & self, IntArrayRef dims);
TORCH_API Tensor transpose(const Tensor & self, Dimname dim0, Dimname dim1);
TORCH_API Tensor one_hot(const Tensor & self, int64_t num_classes=-1);
TORCH_API Tensor fliplr(const Tensor & self);
TORCH_API Tensor flipud(const Tensor & self);
TORCH_API Tensor trapz(const Tensor & y, const Tensor & x, int64_t dim=-1);
TORCH_API Tensor trapz(const Tensor & y, double dx=1, int64_t dim=-1);
TORCH_API Tensor triplet_margin_loss(const Tensor & anchor, const Tensor & positive, const Tensor & negative, double margin=1.0, double p=2, double eps=1e-06, bool swap=false, int64_t reduction=at::Reduction::Mean);
TORCH_API Tensor fix(const Tensor & self);
TORCH_API Tensor & fix_(Tensor & self);
TORCH_API Tensor & fix_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & fix_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor type_as(const Tensor & self, const Tensor & other);
TORCH_API bool _has_compatible_shallow_copy_type(const Tensor & self, const Tensor & from);
TORCH_API Tensor vander(const Tensor & x, c10::optional<int64_t> N=c10::nullopt, bool increasing=false);
TORCH_API Tensor var(const Tensor & self, DimnameList dim, bool unbiased=true, bool keepdim=false);
TORCH_API Tensor & var_out(Tensor & out, const Tensor & self, DimnameList dim, bool unbiased=true, bool keepdim=false);
TORCH_API Tensor & var_outf(const Tensor & self, DimnameList dim, bool unbiased, bool keepdim, Tensor & out);
TORCH_API std::tuple<Tensor,Tensor> var_mean(const Tensor & self, DimnameList dim, bool unbiased=true, bool keepdim=false);
TORCH_API Tensor view_as(const Tensor & self, const Tensor & other);
TORCH_API Tensor where(const Tensor & condition, const Tensor & self, const Tensor & other);
TORCH_API Tensor where(const Tensor & condition, const Scalar & self, const Tensor & other);
TORCH_API Tensor where(const Tensor & condition, const Tensor & self, const Scalar & other);
TORCH_API Tensor where(const Tensor & condition, const Scalar & self, const Scalar & other);
TORCH_API std::vector<Tensor> where(const Tensor & condition);
TORCH_API Tensor norm_except_dim(const Tensor & v, int64_t pow=2, int64_t dim=0);
TORCH_API Tensor _weight_norm(const Tensor & v, const Tensor & g, int64_t dim=0);
TORCH_API std::tuple<Tensor,Tensor> _weight_norm_differentiable_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim);
TORCH_API Tensor zeros(IntArrayRef size, c10::optional<DimnameList> names, TensorOptions options={});
TORCH_API Tensor zeros(IntArrayRef size, c10::optional<DimnameList> names, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor zeros(IntArrayRef size, TensorOptions options={});
TORCH_API Tensor zeros(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & zeros_out(Tensor & out, IntArrayRef size);
TORCH_API Tensor & zeros_outf(IntArrayRef size, Tensor & out);
TORCH_API Tensor zeros_like(const Tensor & self, TensorOptions options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor zeros_like(const Tensor & self, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor _sparse_sum(const Tensor & self);
TORCH_API Tensor _sparse_sum(const Tensor & self, ScalarType dtype);
TORCH_API Tensor _sparse_sum(const Tensor & self, IntArrayRef dim, ScalarType dtype);
TORCH_API Tensor _sparse_softmax(const Tensor & self, int64_t dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor _sparse_softmax(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor _sparse_log_softmax(const Tensor & self, int64_t dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor _sparse_log_softmax(const Tensor & self, Dimname dim, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor norm(const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim, ScalarType dtype);
TORCH_API Tensor & norm_out(Tensor & out, const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim, ScalarType dtype);
TORCH_API Tensor & norm_outf(const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim, ScalarType dtype, Tensor & out);
TORCH_API Tensor norm(const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim=false);
TORCH_API Tensor & norm_out(Tensor & out, const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim=false);
TORCH_API Tensor & norm_outf(const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim, Tensor & out);
TORCH_API Tensor frobenius_norm(const Tensor & self);
TORCH_API Tensor frobenius_norm(const Tensor & self, IntArrayRef dim, bool keepdim=false);
TORCH_API Tensor & frobenius_norm_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim=false);
TORCH_API Tensor & frobenius_norm_outf(const Tensor & self, IntArrayRef dim, bool keepdim, Tensor & out);
TORCH_API Tensor nuclear_norm(const Tensor & self, bool keepdim=false);
TORCH_API Tensor & nuclear_norm_out(Tensor & out, const Tensor & self, bool keepdim=false);
TORCH_API Tensor & nuclear_norm_outf(const Tensor & self, bool keepdim, Tensor & out);
TORCH_API Tensor nuclear_norm(const Tensor & self, IntArrayRef dim, bool keepdim=false);
TORCH_API Tensor & nuclear_norm_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim=false);
TORCH_API Tensor & nuclear_norm_outf(const Tensor & self, IntArrayRef dim, bool keepdim, Tensor & out);
TORCH_API Tensor & subtract_out(Tensor & out, const Tensor & self, const Tensor & other, const Scalar & alpha=1);
TORCH_API Tensor & subtract_outf(const Tensor & self, const Tensor & other, const Scalar & alpha, Tensor & out);
TORCH_API Tensor subtract(const Tensor & self, const Tensor & other, const Scalar & alpha=1);
TORCH_API Tensor & subtract_(Tensor & self, const Tensor & other, const Scalar & alpha=1);
TORCH_API Tensor subtract(const Tensor & self, const Scalar & other, const Scalar & alpha=1);
TORCH_API Tensor & subtract_(Tensor & self, const Scalar & other, const Scalar & alpha=1);
TORCH_API Tensor heaviside(const Tensor & self, const Tensor & values);
TORCH_API Tensor & heaviside_(Tensor & self, const Tensor & values);
TORCH_API Tensor sparse_coo_tensor(IntArrayRef size, TensorOptions options);
TORCH_API Tensor sparse_coo_tensor(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor sparse_coo_tensor(const Tensor & indices, const Tensor & values, TensorOptions options={});
TORCH_API Tensor sparse_coo_tensor(const Tensor & indices, const Tensor & values, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor sparse_coo_tensor(const Tensor & indices, const Tensor & values, IntArrayRef size, TensorOptions options={});
TORCH_API Tensor sparse_coo_tensor(const Tensor & indices, const Tensor & values, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor _sparse_coo_tensor_unsafe(const Tensor & indices, const Tensor & values, IntArrayRef size, TensorOptions options={});
TORCH_API Tensor _sparse_coo_tensor_unsafe(const Tensor & indices, const Tensor & values, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API void _validate_sparse_coo_tensor_args(const Tensor & indices, const Tensor & values, IntArrayRef size);
TORCH_API Tensor to_dense_backward(const Tensor & grad, const Tensor & input);
TORCH_API std::vector<Tensor> unbind(const Tensor & self, Dimname dim);
TORCH_API Tensor to_mkldnn_backward(const Tensor & grad, const Tensor & input);
TORCH_API Tensor fake_quantize_per_tensor_affine(const Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max);
TORCH_API Tensor fake_quantize_per_tensor_affine_cachemask_backward(const Tensor & grad, const Tensor & mask);
TORCH_API std::tuple<Tensor,Tensor,Tensor> _fake_quantize_learnable_per_tensor_affine_backward(const Tensor & grad, const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor=1.0);
TORCH_API Tensor fake_quantize_per_channel_affine(const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max);
TORCH_API Tensor fake_quantize_per_channel_affine_cachemask_backward(const Tensor & grad, const Tensor & mask);
TORCH_API std::tuple<Tensor,Tensor,Tensor> _fake_quantize_learnable_per_channel_affine_backward(const Tensor & grad, const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor=1.0);
TORCH_API std::tuple<double,int64_t> _choose_qparams_per_tensor(const Tensor & self, bool reduce_range=false);
TORCH_API Tensor _saturate_weight_to_fp16(const Tensor & weight);
TORCH_API std::tuple<Tensor,Tensor> choose_qparams_optimized(const Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width);
TORCH_API Tensor to(const Tensor & self, TensorOptions options={}, bool non_blocking=false, bool copy=false, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor to(const Tensor & self, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<MemoryFormat> memory_format);
TORCH_API Tensor to(const Tensor & self, Device device, ScalarType dtype, bool non_blocking=false, bool copy=false, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor to(const Tensor & self, ScalarType dtype, bool non_blocking=false, bool copy=false, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API Tensor to(const Tensor & self, const Tensor & other, bool non_blocking=false, bool copy=false, c10::optional<MemoryFormat> memory_format=c10::nullopt);
TORCH_API std::vector<Tensor> meshgrid(TensorList tensors);
TORCH_API Tensor cartesian_prod(TensorList tensors);
TORCH_API Tensor combinations(const Tensor & self, int64_t r=2, bool with_replacement=false);
TORCH_API Scalar item(const Tensor & self);
TORCH_API ScalarType result_type(const Tensor & tensor, const Tensor & other);
TORCH_API ScalarType result_type(const Tensor & tensor, const Scalar & other);
TORCH_API ScalarType result_type(const Scalar & scalar, const Tensor & tensor);
TORCH_API ScalarType result_type(const Scalar & scalar1, const Scalar & scalar2);
TORCH_API bool can_cast(ScalarType from, ScalarType to);
TORCH_API ScalarType promote_types(ScalarType type1, ScalarType type2);
TORCH_API std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_differentiable_lstm_cell_backward(const c10::optional<Tensor> & grad_hy, const c10::optional<Tensor> & grad_cy, const Tensor & input_gates, const Tensor & hidden_gates, const c10::optional<Tensor> & input_bias, const c10::optional<Tensor> & hidden_bias, const Tensor & cx, const Tensor & cy);
TORCH_API std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_differentiable_gru_cell_backward(const Tensor & grad_hy, const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const c10::optional<Tensor> & input_bias, const c10::optional<Tensor> & hidden_bias);
TORCH_API std::tuple<Tensor,Tensor,Tensor> lstm(const Tensor & input, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
TORCH_API std::tuple<Tensor,Tensor,Tensor> lstm(const Tensor & data, const Tensor & batch_sizes, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
TORCH_API std::tuple<Tensor,Tensor> gru(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
TORCH_API std::tuple<Tensor,Tensor> gru(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
TORCH_API std::tuple<Tensor,Tensor> rnn_tanh(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
TORCH_API std::tuple<Tensor,Tensor> rnn_tanh(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
TORCH_API std::tuple<Tensor,Tensor> rnn_relu(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
TORCH_API std::tuple<Tensor,Tensor> rnn_relu(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
TORCH_API std::tuple<Tensor,Tensor> lstm_cell(const Tensor & input, TensorList hx, const Tensor & w_ih, const Tensor & w_hh, const c10::optional<Tensor> & b_ih={}, const c10::optional<Tensor> & b_hh={});
TORCH_API Tensor gru_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const c10::optional<Tensor> & b_ih={}, const c10::optional<Tensor> & b_hh={});
TORCH_API Tensor rnn_tanh_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const c10::optional<Tensor> & b_ih={}, const c10::optional<Tensor> & b_hh={});
TORCH_API Tensor rnn_relu_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const c10::optional<Tensor> & b_ih={}, const c10::optional<Tensor> & b_hh={});
TORCH_API std::tuple<Tensor,Tensor> quantized_lstm_cell(const Tensor & input, TensorList hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, const Scalar & scale_ih, const Scalar & scale_hh, const Scalar & zero_point_ih, const Scalar & zero_point_hh);
TORCH_API Tensor quantized_gru_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, const Scalar & scale_ih, const Scalar & scale_hh, const Scalar & zero_point_ih, const Scalar & zero_point_hh);
TORCH_API Tensor quantized_rnn_relu_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, const Scalar & scale_ih, const Scalar & scale_hh, const Scalar & zero_point_ih, const Scalar & zero_point_hh);
TORCH_API Tensor quantized_rnn_tanh_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, const Scalar & scale_ih, const Scalar & scale_hh, const Scalar & zero_point_ih, const Scalar & zero_point_hh);
TORCH_API Tensor _pack_padded_sequence_backward(const Tensor & grad, IntArrayRef input_size, const Tensor & batch_sizes, bool batch_first);
TORCH_API std::tuple<Tensor,Tensor> _pad_packed_sequence(const Tensor & data, const Tensor & batch_sizes, bool batch_first, const Scalar & padding_value, int64_t total_length);
TORCH_API Tensor masked_fill(const Tensor & self, const Tensor & mask, const Scalar & value);
TORCH_API Tensor masked_fill(const Tensor & self, const Tensor & mask, const Tensor & value);
TORCH_API Tensor masked_scatter(const Tensor & self, const Tensor & mask, const Tensor & source);
TORCH_API Tensor index_add(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source);
TORCH_API Tensor index_add(const Tensor & self, Dimname dim, const Tensor & index, const Tensor & source);
TORCH_API Tensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Scalar & value);
TORCH_API Tensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & value);
TORCH_API Tensor & index_fill_(Tensor & self, Dimname dim, const Tensor & index, const Scalar & value);
TORCH_API Tensor index_fill(const Tensor & self, Dimname dim, const Tensor & index, const Scalar & value);
TORCH_API Tensor & index_fill_(Tensor & self, Dimname dim, const Tensor & index, const Tensor & value);
TORCH_API Tensor index_fill(const Tensor & self, Dimname dim, const Tensor & index, const Tensor & value);
TORCH_API Tensor scatter(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & src);
TORCH_API Tensor scatter(const Tensor & self, int64_t dim, const Tensor & index, const Scalar & value);
TORCH_API Tensor scatter(const Tensor & self, Dimname dim, const Tensor & index, const Tensor & src);
TORCH_API Tensor scatter(const Tensor & self, Dimname dim, const Tensor & index, const Scalar & value);
TORCH_API Tensor scatter_add(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & src);
TORCH_API Tensor scatter_add(const Tensor & self, Dimname dim, const Tensor & index, const Tensor & src);
TORCH_API Tensor bitwise_and(const Tensor & self, const Tensor & other);
TORCH_API Tensor & bitwise_and_(Tensor & self, const Tensor & other);
TORCH_API Tensor bitwise_and(const Tensor & self, const Scalar & other);
TORCH_API Tensor & bitwise_and_(Tensor & self, const Scalar & other);
TORCH_API Tensor __and__(const Tensor & self, const Scalar & other);
TORCH_API Tensor & __iand__(Tensor & self, const Scalar & other);
TORCH_API Tensor __and__(const Tensor & self, const Tensor & other);
TORCH_API Tensor & __iand__(Tensor & self, const Tensor & other);
TORCH_API Tensor bitwise_or(const Tensor & self, const Tensor & other);
TORCH_API Tensor & bitwise_or_(Tensor & self, const Tensor & other);
TORCH_API Tensor bitwise_or(const Tensor & self, const Scalar & other);
TORCH_API Tensor & bitwise_or_(Tensor & self, const Scalar & other);
TORCH_API Tensor __or__(const Tensor & self, const Scalar & other);
TORCH_API Tensor & __ior__(Tensor & self, const Scalar & other);
TORCH_API Tensor __or__(const Tensor & self, const Tensor & other);
TORCH_API Tensor & __ior__(Tensor & self, const Tensor & other);
TORCH_API Tensor bitwise_xor(const Tensor & self, const Tensor & other);
TORCH_API Tensor & bitwise_xor_(Tensor & self, const Tensor & other);
TORCH_API Tensor bitwise_xor(const Tensor & self, const Scalar & other);
TORCH_API Tensor & bitwise_xor_(Tensor & self, const Scalar & other);
TORCH_API Tensor __xor__(const Tensor & self, const Scalar & other);
TORCH_API Tensor & __ixor__(Tensor & self, const Scalar & other);
TORCH_API Tensor __xor__(const Tensor & self, const Tensor & other);
TORCH_API Tensor & __ixor__(Tensor & self, const Tensor & other);
TORCH_API Tensor & polygamma_(Tensor & self, int64_t n);
TORCH_API Tensor diag_backward(const Tensor & grad, IntArrayRef input_sizes, int64_t diagonal);
TORCH_API Tensor trace_backward(const Tensor & grad, IntArrayRef sizes);
TORCH_API Tensor & not_equal_out(Tensor & out, const Tensor & self, const Scalar & other);
TORCH_API Tensor & not_equal_outf(const Tensor & self, const Scalar & other, Tensor & out);
TORCH_API Tensor not_equal(const Tensor & self, const Scalar & other);
TORCH_API Tensor & not_equal_(Tensor & self, const Scalar & other);
TORCH_API Tensor & not_equal_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & not_equal_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor not_equal(const Tensor & self, const Tensor & other);
TORCH_API Tensor & not_equal_(Tensor & self, const Tensor & other);
TORCH_API Tensor & greater_equal_out(Tensor & out, const Tensor & self, const Scalar & other);
TORCH_API Tensor & greater_equal_outf(const Tensor & self, const Scalar & other, Tensor & out);
TORCH_API Tensor greater_equal(const Tensor & self, const Scalar & other);
TORCH_API Tensor & greater_equal_(Tensor & self, const Scalar & other);
TORCH_API Tensor & greater_equal_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & greater_equal_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor greater_equal(const Tensor & self, const Tensor & other);
TORCH_API Tensor & greater_equal_(Tensor & self, const Tensor & other);
TORCH_API Tensor & less_equal_out(Tensor & out, const Tensor & self, const Scalar & other);
TORCH_API Tensor & less_equal_outf(const Tensor & self, const Scalar & other, Tensor & out);
TORCH_API Tensor less_equal(const Tensor & self, const Scalar & other);
TORCH_API Tensor & less_equal_(Tensor & self, const Scalar & other);
TORCH_API Tensor & less_equal_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & less_equal_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor less_equal(const Tensor & self, const Tensor & other);
TORCH_API Tensor & less_equal_(Tensor & self, const Tensor & other);
TORCH_API Tensor & greater_out(Tensor & out, const Tensor & self, const Scalar & other);
TORCH_API Tensor & greater_outf(const Tensor & self, const Scalar & other, Tensor & out);
TORCH_API Tensor greater(const Tensor & self, const Scalar & other);
TORCH_API Tensor & greater_(Tensor & self, const Scalar & other);
TORCH_API Tensor & greater_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & greater_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor greater(const Tensor & self, const Tensor & other);
TORCH_API Tensor & greater_(Tensor & self, const Tensor & other);
TORCH_API Tensor & less_out(Tensor & out, const Tensor & self, const Scalar & other);
TORCH_API Tensor & less_outf(const Tensor & self, const Scalar & other, Tensor & out);
TORCH_API Tensor less(const Tensor & self, const Scalar & other);
TORCH_API Tensor & less_(Tensor & self, const Scalar & other);
TORCH_API Tensor & less_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & less_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor less(const Tensor & self, const Tensor & other);
TORCH_API Tensor & less_(Tensor & self, const Tensor & other);
TORCH_API Tensor take_backward(const Tensor & grad, const Tensor & input, const Tensor & index);
TORCH_API Tensor & index_select_out(Tensor & out, const Tensor & self, Dimname dim, const Tensor & index);
TORCH_API Tensor & index_select_outf(const Tensor & self, Dimname dim, const Tensor & index, Tensor & out);
TORCH_API Tensor index_select(const Tensor & self, Dimname dim, const Tensor & index);
TORCH_API Tensor index_select_backward(const Tensor & grad, IntArrayRef self_sizes, int64_t dim, const Tensor & index);
TORCH_API Tensor masked_select_backward(const Tensor & grad, const Tensor & input, const Tensor & mask);
TORCH_API std::vector<Tensor> nonzero_numpy(const Tensor & self);
TORCH_API Tensor gather_backward(const Tensor & grad, const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad);
TORCH_API Tensor & gather_out(Tensor & out, const Tensor & self, Dimname dim, const Tensor & index, bool sparse_grad=false);
TORCH_API Tensor & gather_outf(const Tensor & self, Dimname dim, const Tensor & index, bool sparse_grad, Tensor & out);
TORCH_API Tensor gather(const Tensor & self, Dimname dim, const Tensor & index, bool sparse_grad=false);
TORCH_API Tensor _gather_sparse_backward(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & grad);
TORCH_API std::tuple<Tensor &,Tensor &,Tensor &> svd_out(Tensor & U, Tensor & S, Tensor & V, const Tensor & self, bool some=true, bool compute_uv=true);
TORCH_API std::tuple<Tensor &,Tensor &,Tensor &> svd_outf(const Tensor & self, bool some, bool compute_uv, Tensor & U, Tensor & S, Tensor & V);
TORCH_API std::tuple<Tensor,Tensor,Tensor> svd(const Tensor & self, bool some=true, bool compute_uv=true);
TORCH_API Tensor swapaxes(const Tensor & self, int64_t axis0, int64_t axis1);
TORCH_API Tensor & swapaxes_(Tensor & self, int64_t axis0, int64_t axis1);
TORCH_API Tensor swapdims(const Tensor & self, int64_t dim0, int64_t dim1);
TORCH_API Tensor & swapdims_(Tensor & self, int64_t dim0, int64_t dim1);
TORCH_API std::tuple<Tensor &,Tensor &> qr_out(Tensor & Q, Tensor & R, const Tensor & self, bool some=true);
TORCH_API std::tuple<Tensor &,Tensor &> qr_outf(const Tensor & self, bool some, Tensor & Q, Tensor & R);
TORCH_API std::tuple<Tensor,Tensor> qr(const Tensor & self, bool some=true);
TORCH_API Tensor signbit(const Tensor & self);
TORCH_API Tensor max(const Tensor & self, const Tensor & other);
TORCH_API Tensor & max_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & max_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor & min_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & min_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor min(const Tensor & self, const Tensor & other);
TORCH_API Tensor & quantile_out(Tensor & out, const Tensor & self, double q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API Tensor & quantile_outf(const Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, Tensor & out);
TORCH_API Tensor quantile(const Tensor & self, double q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API Tensor & quantile_out(Tensor & out, const Tensor & self, const Tensor & q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API Tensor & quantile_outf(const Tensor & self, const Tensor & q, c10::optional<int64_t> dim, bool keepdim, Tensor & out);
TORCH_API Tensor quantile(const Tensor & self, const Tensor & q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API Tensor & nanquantile_out(Tensor & out, const Tensor & self, double q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API Tensor & nanquantile_outf(const Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, Tensor & out);
TORCH_API Tensor nanquantile(const Tensor & self, double q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API Tensor & nanquantile_out(Tensor & out, const Tensor & self, const Tensor & q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API Tensor & nanquantile_outf(const Tensor & self, const Tensor & q, c10::optional<int64_t> dim, bool keepdim, Tensor & out);
TORCH_API Tensor nanquantile(const Tensor & self, const Tensor & q, c10::optional<int64_t> dim=c10::nullopt, bool keepdim=false);
TORCH_API std::tuple<Tensor &,Tensor &> sort_out(Tensor & values, Tensor & indices, const Tensor & self, Dimname dim, bool descending=false);
TORCH_API std::tuple<Tensor &,Tensor &> sort_outf(const Tensor & self, Dimname dim, bool descending, Tensor & values, Tensor & indices);
TORCH_API std::tuple<Tensor,Tensor> sort(const Tensor & self, Dimname dim, bool descending=false);
TORCH_API std::tuple<Tensor &,Tensor &> sort_out(Tensor & values, Tensor & indices, const Tensor & self, c10::optional<bool> stable, Dimname dim, bool descending=false);
TORCH_API std::tuple<Tensor &,Tensor &> sort_outf(const Tensor & self, c10::optional<bool> stable, Dimname dim, bool descending, Tensor & values, Tensor & indices);
TORCH_API std::tuple<Tensor,Tensor> sort(const Tensor & self, c10::optional<bool> stable, Dimname dim, bool descending=false);
TORCH_API Tensor & msort_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & msort_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor msort(const Tensor & self);
TORCH_API Tensor argsort(const Tensor & self, int64_t dim=-1, bool descending=false);
TORCH_API Tensor argsort(const Tensor & self, Dimname dim, bool descending=false);
TORCH_API Tensor & float_power_out(Tensor & out, const Tensor & self, const Tensor & exponent);
TORCH_API Tensor & float_power_outf(const Tensor & self, const Tensor & exponent, Tensor & out);
TORCH_API Tensor float_power(const Tensor & self, const Tensor & exponent);
TORCH_API Tensor & float_power_(Tensor & self, const Tensor & exponent);
TORCH_API Tensor & float_power_out(Tensor & out, const Scalar & self, const Tensor & exponent);
TORCH_API Tensor & float_power_outf(const Scalar & self, const Tensor & exponent, Tensor & out);
TORCH_API Tensor float_power(const Scalar & self, const Tensor & exponent);
TORCH_API Tensor & float_power_out(Tensor & out, const Tensor & self, const Scalar & exponent);
TORCH_API Tensor & float_power_outf(const Tensor & self, const Scalar & exponent, Tensor & out);
TORCH_API Tensor float_power(const Tensor & self, const Scalar & exponent);
TORCH_API Tensor & float_power_(Tensor & self, const Scalar & exponent);
TORCH_API Tensor normal(double mean, double std, IntArrayRef size, c10::optional<Generator> generator=c10::nullopt, TensorOptions options={});
TORCH_API Tensor normal(double mean, double std, IntArrayRef size, c10::optional<Generator> generator, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & normal_out(Tensor & out, double mean, double std, IntArrayRef size, c10::optional<Generator> generator=c10::nullopt);
TORCH_API Tensor & normal_outf(double mean, double std, IntArrayRef size, c10::optional<Generator> generator, Tensor & out);
TORCH_API Tensor & multilabel_margin_loss_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction=at::Reduction::Mean);
TORCH_API Tensor & multilabel_margin_loss_outf(const Tensor & self, const Tensor & target, int64_t reduction, Tensor & out);
TORCH_API Tensor multilabel_margin_loss(const Tensor & self, const Tensor & target, int64_t reduction=at::Reduction::Mean);
TORCH_API Tensor & nll_loss_out(Tensor & out, const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight={}, int64_t reduction=at::Reduction::Mean, int64_t ignore_index=-100);
TORCH_API Tensor & nll_loss_outf(const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight, int64_t reduction, int64_t ignore_index, Tensor & out);
TORCH_API Tensor nll_loss(const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight={}, int64_t reduction=at::Reduction::Mean, int64_t ignore_index=-100);
TORCH_API Tensor & nll_loss2d_out(Tensor & out, const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight={}, int64_t reduction=at::Reduction::Mean, int64_t ignore_index=-100);
TORCH_API Tensor & nll_loss2d_outf(const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight, int64_t reduction, int64_t ignore_index, Tensor & out);
TORCH_API Tensor nll_loss2d(const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight={}, int64_t reduction=at::Reduction::Mean, int64_t ignore_index=-100);
TORCH_API Tensor & log_sigmoid_out(Tensor & out, const Tensor & self);
TORCH_API Tensor & log_sigmoid_outf(const Tensor & self, Tensor & out);
TORCH_API Tensor log_sigmoid(const Tensor & self);
TORCH_API Tensor adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size);
TORCH_API Tensor & thnn_conv2d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0);
TORCH_API Tensor & thnn_conv2d_outf(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, Tensor & out);
TORCH_API Tensor thnn_conv2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0);
TORCH_API Tensor & thnn_conv_depthwise2d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef dilation=1);
TORCH_API Tensor & thnn_conv_depthwise2d_outf(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, Tensor & out);
TORCH_API Tensor thnn_conv_depthwise2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0, IntArrayRef dilation=1);
TORCH_API Tensor & slow_conv3d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0);
TORCH_API Tensor & slow_conv3d_outf(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, Tensor & out);
TORCH_API Tensor slow_conv3d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias={}, IntArrayRef stride=1, IntArrayRef padding=0);
TORCH_API Tensor column_stack(TensorList tensors);
TORCH_API Tensor & column_stack_out(Tensor & out, TensorList tensors);
TORCH_API Tensor & column_stack_outf(TensorList tensors, Tensor & out);
TORCH_API Tensor isfinite(const Tensor & self);
TORCH_API Tensor isinf(const Tensor & self);
TORCH_API Tensor isposinf(const Tensor & self);
TORCH_API Tensor isneginf(const Tensor & self);
TORCH_API Tensor _add_batch_dim(const Tensor & self, int64_t batch_dim, int64_t level);
TORCH_API Tensor _remove_batch_dim(const Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim);
TORCH_API Tensor special_gammaln(const Tensor & self);
TORCH_API Tensor fft_fft(const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_fft_out(Tensor & out, const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_fft_outf(const Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_ifft(const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ifft_out(Tensor & out, const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ifft_outf(const Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_rfft(const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_rfft_out(Tensor & out, const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_rfft_outf(const Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_irfft(const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_irfft_out(Tensor & out, const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_irfft_outf(const Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_hfft(const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_hfft_out(Tensor & out, const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_hfft_outf(const Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_ihfft(const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ihfft_out(Tensor & out, const Tensor & self, c10::optional<int64_t> n=c10::nullopt, int64_t dim=-1, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ihfft_outf(const Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_fft2(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_fft2_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_fft2_outf(const Tensor & self, c10::optional<IntArrayRef> s, IntArrayRef dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_ifft2(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ifft2_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ifft2_outf(const Tensor & self, c10::optional<IntArrayRef> s, IntArrayRef dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_rfft2(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_rfft2_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_rfft2_outf(const Tensor & self, c10::optional<IntArrayRef> s, IntArrayRef dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_irfft2(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_irfft2_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, IntArrayRef dim={-2,-1}, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_irfft2_outf(const Tensor & self, c10::optional<IntArrayRef> s, IntArrayRef dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_fftn(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_fftn_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_fftn_outf(const Tensor & self, c10::optional<IntArrayRef> s, c10::optional<IntArrayRef> dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_ifftn(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ifftn_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_ifftn_outf(const Tensor & self, c10::optional<IntArrayRef> s, c10::optional<IntArrayRef> dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_rfftn(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_rfftn_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_rfftn_outf(const Tensor & self, c10::optional<IntArrayRef> s, c10::optional<IntArrayRef> dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_irfftn(const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_irfftn_out(Tensor & out, const Tensor & self, c10::optional<IntArrayRef> s=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, c10::optional<std::string> norm=c10::nullopt);
TORCH_API Tensor & fft_irfftn_outf(const Tensor & self, c10::optional<IntArrayRef> s, c10::optional<IntArrayRef> dim, c10::optional<std::string> norm, Tensor & out);
TORCH_API Tensor fft_fftfreq(int64_t n, double d=1.0, TensorOptions options={});
TORCH_API Tensor fft_fftfreq(int64_t n, double d, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & fft_fftfreq_out(Tensor & out, int64_t n, double d=1.0);
TORCH_API Tensor & fft_fftfreq_outf(int64_t n, double d, Tensor & out);
TORCH_API Tensor fft_rfftfreq(int64_t n, double d=1.0, TensorOptions options={});
TORCH_API Tensor fft_rfftfreq(int64_t n, double d, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory);
TORCH_API Tensor & fft_rfftfreq_out(Tensor & out, int64_t n, double d=1.0);
TORCH_API Tensor & fft_rfftfreq_outf(int64_t n, double d, Tensor & out);
TORCH_API Tensor fft_fftshift(const Tensor & self, c10::optional<IntArrayRef> dim=c10::nullopt);
TORCH_API Tensor fft_ifftshift(const Tensor & self, c10::optional<IntArrayRef> dim=c10::nullopt);
TORCH_API Tensor linalg_det(const Tensor & self);
TORCH_API Tensor inner(const Tensor & self, const Tensor & other);
TORCH_API Tensor & inner_out(Tensor & out, const Tensor & self, const Tensor & other);
TORCH_API Tensor & inner_outf(const Tensor & self, const Tensor & other, Tensor & out);
TORCH_API Tensor outer(const Tensor & self, const Tensor & vec2);
TORCH_API Tensor & outer_out(Tensor & out, const Tensor & self, const Tensor & vec2);
TORCH_API Tensor & outer_outf(const Tensor & self, const Tensor & vec2, Tensor & out);
TORCH_API Tensor linalg_norm(const Tensor & self, const c10::optional<Scalar> & ord=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & linalg_norm_out(Tensor & out, const Tensor & self, const c10::optional<Scalar> & ord=c10::nullopt, c10::optional<IntArrayRef> dim=c10::nullopt, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & linalg_norm_outf(const Tensor & self, const c10::optional<Scalar> & ord, c10::optional<IntArrayRef> dim, bool keepdim, c10::optional<ScalarType> dtype, Tensor & out);
TORCH_API Tensor linalg_norm(const Tensor & self, std::string ord, c10::optional<IntArrayRef> dim=c10::nullopt, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & linalg_norm_out(Tensor & out, const Tensor & self, std::string ord, c10::optional<IntArrayRef> dim=c10::nullopt, bool keepdim=false, c10::optional<ScalarType> dtype=c10::nullopt);
TORCH_API Tensor & linalg_norm_outf(const Tensor & self, std::string ord, c10::optional<IntArrayRef> dim, bool keepdim, c10::optional<ScalarType> dtype, Tensor & out);
TORCH_API std::tuple<Tensor &,Tensor &,Tensor &> linalg_svd_out(Tensor & U, Tensor & S, Tensor & V, const Tensor & self, bool full_matrices=true, bool compute_uv=true);
TORCH_API std::tuple<Tensor &,Tensor &,Tensor &> linalg_svd_outf(const Tensor & self, bool full_matrices, bool compute_uv, Tensor & U, Tensor & S, Tensor & V);
TORCH_API std::tuple<Tensor,Tensor,Tensor> linalg_svd(const Tensor & self, bool full_matrices=true, bool compute_uv=true);
TORCH_API Tensor linalg_cond(const Tensor & self, const c10::optional<Scalar> & p=c10::nullopt);
TORCH_API Tensor & linalg_cond_out(Tensor & out, const Tensor & self, const c10::optional<Scalar> & p=c10::nullopt);
TORCH_API Tensor & linalg_cond_outf(const Tensor & self, const c10::optional<Scalar> & p, Tensor & out);
TORCH_API Tensor linalg_cond(const Tensor & self, std::string p);
TORCH_API Tensor & linalg_cond_out(Tensor & out, const Tensor & self, std::string p);
TORCH_API Tensor & linalg_cond_outf(const Tensor & self, std::string p, Tensor & out);
TORCH_API Tensor linalg_pinv(const Tensor & self, double rcond=1e-15, bool hermitian=false);
TORCH_API Tensor & linalg_pinv_out(Tensor & out, const Tensor & self, double rcond=1e-15, bool hermitian=false);
TORCH_API Tensor & linalg_pinv_outf(const Tensor & self, double rcond, bool hermitian, Tensor & out);
TORCH_API Tensor linalg_pinv(const Tensor & self, const Tensor & rcond, bool hermitian=false);
TORCH_API Tensor & linalg_pinv_out(Tensor & out, const Tensor & self, const Tensor & rcond, bool hermitian=false);
TORCH_API Tensor & linalg_pinv_outf(const Tensor & self, const Tensor & rcond, bool hermitian, Tensor & out);
TORCH_API Tensor linalg_tensorinv(const Tensor & self, int64_t ind=2);
TORCH_API Tensor & linalg_tensorinv_out(Tensor & out, const Tensor & self, int64_t ind=2);
TORCH_API Tensor & linalg_tensorinv_outf(const Tensor & self, int64_t ind, Tensor & out);
TORCH_API Tensor linalg_tensorsolve(const Tensor & self, const Tensor & other, c10::optional<IntArrayRef> dims=c10::nullopt);
TORCH_API Tensor & linalg_tensorsolve_out(Tensor & out, const Tensor & self, const Tensor & other, c10::optional<IntArrayRef> dims=c10::nullopt);
TORCH_API Tensor & linalg_tensorsolve_outf(const Tensor & self, const Tensor & other, c10::optional<IntArrayRef> dims, Tensor & out);
TORCH_API Tensor linalg_matrix_rank(const Tensor & self, c10::optional<double> tol=c10::nullopt, bool hermitian=false);
TORCH_API Tensor & linalg_matrix_rank_out(Tensor & out, const Tensor & self, c10::optional<double> tol=c10::nullopt, bool hermitian=false);
TORCH_API Tensor & linalg_matrix_rank_outf(const Tensor & self, c10::optional<double> tol, bool hermitian, Tensor & out);
TORCH_API Tensor _test_serialization_subcmul(const Tensor & self, const Tensor & other, const Scalar & alpha=1);
TORCH_API Tensor _test_string_default(const Tensor & dummy, std::string a="\"'\\", std::string b="\"'\\");
TORCH_API Tensor _test_ambiguous_defaults(const Tensor & dummy, int64_t a=1, int64_t b=1);
TORCH_API Tensor _test_ambiguous_defaults(const Tensor & dummy, int64_t a, std::string b);

} // namespace math
} // namespace at
